<!DOCTYPE html>
<html class="writer-html5" lang="hpopt" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Using hpopt &mdash; hpopt 0.1 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="HpOpt API Reference" href="apis.html" />
    <link rel="prev" title="Getting Started with hpopt" href="getting_started.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> hpopt
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="intro.html">What is hpopt?</a></li>
<li class="toctree-l1"><a class="reference internal" href="install.html">Install</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Getting Started with hpopt</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Using hpopt</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#search-space">Search space</a></li>
<li class="toctree-l2"><a class="reference internal" href="#hpopt-class">hpopt class</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#baye-opt-smbo">Baye_opt (SMBO)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#asha">ASHA</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#implementation-detail">Implementation Detail</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#implement-hpo-loop">Implement HPO loop</a></li>
<li class="toctree-l3"><a class="reference internal" href="#implment-run-hpo-trainer-function">Implment &quot;run_hpo_trainer&quot; function</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="apis.html">HpOpt API Reference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Flowcharts:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="flowchart.html">Flowchart</a></li>
<li class="toctree-l1"><a class="reference internal" href="standard.html">Standard</a></li>
<li class="toctree-l1"><a class="reference internal" href="pause.html">Pause / Resume</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">HPO_OTE:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="hpo_ote.html">HPO with OTE</a></li>
<li class="toctree-l1"><a class="reference internal" href="hpo_config.html">How to write hpo_config.yaml</a></li>
<li class="toctree-l1"><a class="reference internal" href="cls_tutorial.html">Classification with OTE</a></li>
<li class="toctree-l1"><a class="reference internal" href="det_tutorial.html">Detection with OTE</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">hpopt</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Using hpopt</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/contents/using.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="using-hpopt">
<h1>Using hpopt<a class="headerlink" href="#using-hpopt" title="Permalink to this heading"></a></h1>
<p>We'll introduce detail of hpopt in this page.
After reading this page, you can get some tips to use hpopt more elegantly.</p>
<section id="search-space">
<h2>Search space<a class="headerlink" href="#search-space" title="Permalink to this heading"></a></h2>
<p>If you want to run HPO, you should have hyper parameters to optmimize.
And you should make hyper parameter search space with that hyper parameters and use it as hpopt aurgment.
Fortunately, it's not hard to make search space.</p>
<p>Search space should be dictionary which has pairs of key and value.
Key is hyper parameter name you can set freely and value is <strong>hpopt.search_space</strong> class.
<em>search_space</em> class get two arguments when initiated.
First one is a <em>type</em> and second one is a <em>range</em>.
<em>type</em> is used when sampling hyper parameter from search space.
you can not only sample from normal uniform space, but also sample from quantized search space for some hyper parameter.
For example batch size is generally set as even integer number. In this case, you can use quantized search space.
Another type is log scale search space.
You may want to sample hyper parameter from log scale search space.
For this case, we provide log scale with any base you want.
Of course, you can use log scale quantized search space.
You can also use categorical hyper parameters.
For example if you want to optimize optmizer, you can set this by categorical search space.
With this various search space, you can make any search space you want.
For pratical implementation, refer <a class="reference internal" href="apis.html"><span class="doc"></span></a></p>
</section>
<section id="hpopt-class">
<h2>hpopt class<a class="headerlink" href="#hpopt-class" title="Permalink to this heading"></a></h2>
<p>After you make search space, you should initiate hpopt class.</p>
<p>Arguments you can set are slightly different depending on whether using baye_opt or ASHA.
But don't worry. They are just advanced options so you can just leave them as default value.
First of all, let's take a look at some important arguments used for initiating class.</p>
<p>There are three required arguments.</p>
<ul class="simple">
<li><p>full_dataset_size : It's a just train dataset size.</p></li>
<li><p>num_full_iterations : Epoch for training after HPO.</p></li>
<li><p>search_space : hyper parameter search space to optimize.</p></li>
</ul>
<p>You may think that &quot;search_space is ok, but why outhers are required?&quot;.
Actually, one magic hides in this.
<strong>The Auto Config.</strong>
HPO finds the optimal hyper-parameters by trying to train the model with various hyper-parameters.
Because of this behavior, it usually takes a huge amount of time to run HPO, which was the biggest obstacle to using HPO practically.
In order to get good results in appropriate time, the user needs to configure the HPO well.
hpopt has a feature called <strong>Auto Config</strong> that configures parameters of HPO automaticaly.
This feature makes hpopt done in expeceted time.
Auto configurated hyper parameters are as below.</p>
<ul class="simple">
<li><p>num_trials : Number of times to sample from the hyperparameter search space. It should be greater than or equal to 1.</p></li>
<li><p>max_iterations : Max training epoch for each trials. hpopt will stop training after iterating ‘max_iterations’ times.</p></li>
<li><p>subset_ratio : The ratio of dataset size for HPO task. If this value is greater than or equal to 1.0, full dataset is used for HPO task. When it makes dataset size lower than 500, Dataset size is set to 500.</p></li>
</ul>
<p>As you know intuitively, the lower these values are, the faster HPO is.
Auto config is activated when you set <em>expected_time_ratio</em> and don't set those arguments.
If you set some of these arugments, then arguments except what you set are configured automatically.
<em>non_pure_train_ratio</em> is also used to estimate HPO time more accurately.
Those two things are used for autu configuration to estimate HPO time.
You understand why first two are required now.</p>
<div class="highlight-{note} notranslate"><div class="highlight"><pre><span></span>Expected time ratio means how many times you use for the sum of HPO and finetuning compared to just finetuning time.
In other words, if you set expected time ratio to 2, you&#39;ll use same time as training time for HPO.
</pre></div>
</div>
<p><em>search_alg</em> is another important argument. It should be either <em>baye_opt</em> or <em>asha</em>.
<em>baye_opt</em> is sequentail model based optimization(SMBO).
As you can see in the name, <em>baye_opt</em> executes each trial sequentailly.
<em>asha</em> executes each trial in parallel contrariwise.
So, <em>baye_opt</em> is recommeneded if you have few training resources.
Otherwise, <em>asha</em> is recommended.</p>
<p><em>resume</em> is convenient feature of hpopt. If you stopped in the middle of HPO for some reason,
and then you want to resume HPO from stopped point, you can do that by set resume to True.
In this case, you should set same <em>save_path</em> as you set previously.
Then, hpopt automatically succeeds to previous one.
Another conveninent point is decreasing batch size adaptively.
If you set <em>batch_size_name</em> and implement code to report cuda out of memory using hpopt.reportOOM,
hpopt adaptively decrease batch size upper bound and make a new trial.</p>
<section id="baye-opt-smbo">
<h3>Baye_opt (SMBO)<a class="headerlink" href="#baye-opt-smbo" title="Permalink to this heading"></a></h3>
<div class="highlight-{image} ./image/smbo.png notranslate"><div class="highlight"><pre><span></span>:alt: smbo
:scale: 60%
:align: center
:target: https://arxiv.org/pdf/1012.2599v1.pdf
</pre></div>
</div>
<p><em>Image by [<a class="reference external" href="https://arxiv.org/pdf/1012.2599v1.pdf">Brochu et al., 2010</a>]</em></p>
<p>Bayesian optimizaiton is one of HPO methods commoly used.
hpopt uses <a class="reference external" href="https://github.com/fmfn/BayesianOptimization">BayesianOptimization</a> library for SMBO.
It estimates score map from previuos trials using gaussian process,
and chooses next candiate to try.
It's calssic but powerful method.</p>
<p>You can modify behavior of <em>baye_opt</em> by giving some arguments. Here is those arguments.</p>
<ul class="simple">
<li><p>early_stop : Choice of early stopping methods. Currently ‘median_stop’ is supported.</p></li>
<li><p>num_init_trial : How many trials to use to init SMBO.</p></li>
<li><p>kappa : Kappa vlaue for ucb used in bayesian optimization.</p></li>
<li><p>kappa_decay : Multiply kappa by kappa_decay every trials.</p></li>
<li><p>kappa_decay_delay : Kappa isn’t multiplied to kappa_decay from first trials to kappa_decay_delay trials.</p></li>
</ul>
<p>If <em>early_stop</em> is set, some trials which is unlikely to get good score stopped in the middle.
It can save time by skipping inefficient trial.
Shortcoming of early stop is that it could skip trial which get lower score early but best score in the end.</p>
<p>To estimate score map, Bayesian optimization needs some initial trial sampled randomly.
<em>num_init_trial</em> decides how many trials use random sampled hyper parameters.</p>
<p>Bayesian optmization should balances between exploration and exploitation.
To be more sepecific, exploration means choosing unseen sample from search space
and exploitation means choosing sample which is likely going to provide good score.
If exploration overwhelms, bayesian optimization is almost same as random sampling,
In other case, Many good candiates is left unseen. So it's important to balance between them.
<em>kappa</em> is hyper parameter of bayesian optimization.
if higher kappa is higher, bayesian optimization explores more and vice versa.
you can modify this value to change balance.</p>
</section>
<section id="asha">
<h3>ASHA<a class="headerlink" href="#asha" title="Permalink to this heading"></a></h3>
<p>If you have abundant GPU resouces, it must be better to run HPO parallely.
ASHA is good choice in that case.
ASHA runs multiple trials parallely, compares scores between them and terminates bed trials at specific iteration.
Left trials continue training, are compared again and some of them are terminated at next specific iteration and so on.
It's a like tournament.
Maybe you worry about that late bloomer is terminated eariler.
To avoid that case, ASHA run multiple tournaments some of which has very big interval between checkpoionts.</p>
<p>There are some argument defining ASHA behavior like bayesian optimization.</p>
<ul class="simple">
<li><p>min_iterations : hpopt will run training at least ‘min_iterations’ times.</p></li>
<li><p>reduction_factor : Used to set halving rate and amount.</p></li>
<li><p>num_brackets : Number of brackets. Each bracket has a different halving rate, specified by the reduction factor.</p></li>
</ul>
<p><em>min_iteration</em> is minimum iterations to compare scores literally. If you want that trial terminated after at least some iterations, you need to set this.</p>
<p>Only 1/n trials can continue training every checkpoint.
<em>reduction_factor</em> means &quot;n&quot; used in previuos sentence.
That means if n is high, more of them terminates at checkpoint. It is recommended not to set it too high( &gt; 4).
It could make HPO unstable.</p>
<p><em>num_brackets</em> means number of tournaments. More higher this value is, the more various checkpoint interval there are.</p>
</section>
</section>
<section id="implementation-detail">
<h2>Implementation Detail<a class="headerlink" href="#implementation-detail" title="Permalink to this heading"></a></h2>
<p>You need to implement loop to run each HPO trial after making hpopt class.
It's totally up to you how to implement the loop.
Although, I provide sample code expecting it could be simple guide to you.</p>
<section id="implement-hpo-loop">
<h3>Implement HPO loop<a class="headerlink" href="#implement-hpo-loop" title="Permalink to this heading"></a></h3>
<p>Let't figure out how to implement loop code.
In this case, I assume that we use <em>baye_opt</em> as HPO algorithm.
Because the way of preceeding I'll expalain can be applied to all other environments
(one node with a GPU, multi node with multi GPU, etc.),
you are able to implement HPO with any environments.</p>
<p>Please take a look at below code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">hpopt</span>

<span class="k">def</span> <span class="nf">run_hpo_trainer</span><span class="p">(</span><span class="n">train_config</span><span class="p">):</span>
    <span class="o">...</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">hpo</span> <span class="o">=</span> <span class="n">hpopt</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
        <span class="n">search_alg</span> <span class="o">=</span> <span class="s2">&quot;baye_opt&quot;</span><span class="p">,</span>
        <span class="o">...</span>
    <span class="p">)</span>

    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">train_config</span> <span class="o">=</span> <span class="n">hpo</span><span class="o">.</span><span class="n">get_next_sample</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">train_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="n">run_hpo_trainer</span><span class="p">(</span><span class="n">train_config</span><span class="p">)</span>

    <span class="n">best_config</span> <span class="o">=</span> <span class="n">hpo</span><span class="o">.</span><span class="n">get_best_config</span><span class="p">()</span>
    <span class="n">hpo</span><span class="o">.</span><span class="n">print_results</span><span class="p">()</span>
</pre></div>
</div>
<p>That's easy, right?
What you need to do is just getting <em>train_config</em> by hpo.get_next_sample() and train model with them.
I skipped how to implement <em>run_hpo_trainer</em> function now.
I'll explain in a little while.</p>
<p>Do you remember that hpopt can adaptively decrease batch size if batch size is too big to GPU?
To enable this feature, you should add some code a little bit more. Take a look below code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">hpopt</span>

<span class="k">def</span> <span class="nf">run_hpo_trainer</span><span class="p">(</span><span class="n">train_config</span><span class="p">):</span>
    <span class="o">...</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">hpo</span> <span class="o">=</span> <span class="n">hpopt</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
        <span class="n">search_alg</span> <span class="o">=</span> <span class="s2">&quot;baye_opt&quot;</span><span class="p">,</span>
        <span class="n">batch_size_name</span> <span class="o">=</span> <span class="s2">&quot;bs&quot;</span><span class="p">,</span>
        <span class="o">...</span>
    <span class="p">)</span>

    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">train_config</span> <span class="o">=</span> <span class="n">hpo</span><span class="o">.</span><span class="n">get_next_sample</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">train_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">break</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">run_hpo_trainer</span><span class="p">(</span><span class="n">train_config</span><span class="p">)</span>
        <span class="c1"># It can be different depending on your training framework.</span>
        <span class="k">except</span> <span class="ne">RuntimeError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span> 
            <span class="k">if</span> <span class="nb">str</span><span class="p">(</span><span class="n">err</span><span class="p">)</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;CUDA out of memory&quot;</span><span class="p">):</span>
                <span class="n">hpopt</span><span class="o">.</span><span class="n">reportOOM</span><span class="p">(</span><span class="n">train_config</span><span class="p">)</span>

    <span class="n">best_config</span> <span class="o">=</span> <span class="n">hpo</span><span class="o">.</span><span class="n">get_best_config</span><span class="p">()</span>
    <span class="n">hpo</span><span class="o">.</span><span class="n">print_results</span><span class="p">()</span>
</pre></div>
</div>
<p>You can see that <em>batch_size_name</em> is used to initiate hpopt class and
error handling syntax during invoking <em>run_hpo_trainer</em>.
If you report &quot;cuda out of memory&quot; situation to hpopt,
hpopt then decrease batch size search space automatically refering <em>batch_size_name</em>.</p>
</section>
<section id="implment-run-hpo-trainer-function">
<h3>Implment &quot;run_hpo_trainer&quot; function<a class="headerlink" href="#implment-run-hpo-trainer-function" title="Permalink to this heading"></a></h3>
<p>It's time to explain how to implement &quot;run_hpo_trainer&quot; function now.
Task of HPO is to optimize objective function which has input as hyper parameter and score as output.
So you need to make function which can get hyper parameters as input, run training with those hyper parameters and return score.
Do you remember &quot;run_hpo_trainer&quot; get <strong>train_config</strong> returned from <em>hpo.get_best_config()</em>?
You need to understand what <em>train_config</em> is first of all.</p>
<p><strong>train_config</strong> is dictionary which contains below keys.</p>
<ul class="simple">
<li><p>params : hyper parameters to optimize</p></li>
<li><p>iterations : iterations for training</p></li>
<li><p>subset_ratio : train dataset size ratio.</p></li>
</ul>
<p>Actually, there are others not written here.
I only introduce keys used in &quot;run_hpo_trainer&quot;.
Now, what you need to do is to implement function which train models with these train configurations.
Before I show sample code,
please note that I assume that we use torch as framework,
and of course, you can change this code according to yours.
Let's see below code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">hpopt</span>


<span class="k">def</span> <span class="nf">run_hpo_trainer</span><span class="p">(</span><span class="n">train_config</span><span class="p">):</span>
    <span class="n">total_epoch</span> <span class="o">=</span> <span class="n">train_config</span><span class="p">[</span><span class="s1">&#39;iterations&#39;</span><span class="p">]</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">train_config</span><span class="p">[</span><span class="s1">&#39;param&#39;</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    <span class="n">bs</span> <span class="o">=</span> <span class="n">train_config</span><span class="p">[</span><span class="s1">&#39;param&#39;</span><span class="p">][</span><span class="s1">&#39;bs&#39;</span><span class="p">]</span>
    <span class="n">subset_ratio</span> <span class="o">=</span> <span class="n">train_config</span><span class="p">[</span><span class="s1">&#39;subset_ratio&#39;</span><span class="p">]</span>

    <span class="o">...</span>
    <span class="n">train_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">subset_ratio</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">full_dataset</span><span class="p">))</span>
    <span class="n">removed_data</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">full_dataset</span><span class="p">)</span> <span class="o">-</span> <span class="n">train_size</span>
    <span class="n">training_set</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">random_split</span><span class="p">(</span><span class="n">full_dataset</span><span class="p">,</span> <span class="p">[</span><span class="n">train_size</span><span class="p">,</span> <span class="n">removed_data</span><span class="p">])</span>
    <span class="n">training_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">training_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">bs</span><span class="p">)</span>
    <span class="o">...</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
    <span class="o">...</span>

    <span class="k">def</span> <span class="nf">train_one_epoch</span><span class="p">():</span>
        <span class="o">...</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">total_epoch</span><span class="p">):</span>
        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">train_one_epoch</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">hpopt</span><span class="o">.</span><span class="n">report</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">hp_config</span><span class="p">,</span> <span class="n">score</span><span class="o">=</span><span class="n">score</span><span class="p">)</span> <span class="o">==</span> <span class="n">hpopt</span><span class="o">.</span><span class="n">Status</span><span class="o">.</span><span class="n">STOP</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="o">...</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">hpo</span> <span class="o">=</span> <span class="n">hpopt</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">train_config</span> <span class="o">=</span> <span class="n">hpo</span><span class="o">.</span><span class="n">get_next_sample</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">train_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="n">run_hpo_trainer</span><span class="p">(</span><span class="n">train_config</span><span class="p">)</span>

    <span class="n">best_config</span> <span class="o">=</span> <span class="n">hpo</span><span class="o">.</span><span class="n">get_best_config</span><span class="p">()</span>
    <span class="n">hpo</span><span class="o">.</span><span class="n">print_results</span><span class="p">()</span>
</pre></div>
</div>
<p>You can see that lr and bs are set and training set is splitted according to <em>subset_ratio</em>.
After preparation, model is trained with interations given from hpopt.
You shouldn't miss reporting score to hpopt after every epoch.
If hpopt determines that train doesn't need to proceed further,
<em>hpopt.report()</em> returns hpopt.Status.STOP.
Then you can just terminate train.</p>
<p>That's it! you can now use hpopt freely according to your need. Please enjoy!</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="getting_started.html" class="btn btn-neutral float-left" title="Getting Started with hpopt" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="apis.html" class="btn btn-neutral float-right" title="HpOpt API Reference" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, intel.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>